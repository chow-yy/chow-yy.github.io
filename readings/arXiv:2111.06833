<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Frequency Estimation in the Shuffle Model with Almost a Single Message</title>
    
    <!-- MathJax Configuration: Enables inline dollar sign delimiters ($) for LaTeX -->
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
          }
        };
    </script>
    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <!-- Cache Control (Retained for completeness) -->
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
    <meta http-equiv="Pragma" content="no-cache" />
    <meta http-equiv="Expires" content="0" />

    <style>
      :root{--bg:#f9fafb;--card:#ffffff;--muted:#4b5563;--accent:#0284c7}
      body{font-family:Inter,Segoe UI,Roboto,Helvetica,Arial,sans-serif;background:linear-gradient(180deg,#f0f9ff 0%,#ffffff 100%);color:#111827;margin:0;padding:32px}
      .container{max-width:980px;margin:0 auto}
      header{display:flex;align-items:center;gap:16px}
      h1{margin:0;font-size:28px}
      h2,h3,h4{scroll-margin-top:80px}
      a{color:var(--accent);text-decoration:none}
      a:hover{text-decoration:underline}
      p.lead{color:var(--muted);margin-top:8px}
      .card{background:var(--card);border-radius:12px;padding:18px;margin-top:18px;box-shadow:0 6px 24px rgba(0,0,0,0.05)}
      pre{background:#f1f5f9;padding:12px;border-radius:8px;overflow:auto}
      .grid{display:grid;grid-template-columns:1fr 360px;gap:16px}
      label{font-size:13px;color:var(--muted)}
      input,button,select,textarea{padding:8px;border-radius:8px;border:1px solid rgba(0,0,0,0.1);background:#f9fafb;color:inherit}
      button{cursor:pointer;background:var(--accent);color:white;border:none}
      button:hover{background:#0369a1}
      .monospace{font-family:ui-monospace,SFMono-Regular,Menlo,monospace}
      .status{font-size:13px;color:var(--muted)}
      .row{display:flex;gap:8px}
      .output{background:#f1f5f9;padding:12px;border-radius:8px;overflow-x:auto}
      footer{margin-top:28px;color:var(--muted);font-size:13px;text-align:center}
      .note{background:#e0f2fe;padding:10px;border-radius:8px;color:#0369a1}
      .cite-note{margin-top:8px;font-size:11px;color:#6b7280}
      table{border-collapse:collapse;margin-top:12px;width:100%}
      table, th, td{border:1px solid #ccc}
      th, td{padding:6px;text-align:center}
    </style>
</head>
<body>
    <div>
    <h1>Frequency Estimation in the Shuffle Model with Almost a Single Message</h1>
        <p class="note"><strong>Authors:</strong> Qiyao Luo, Yilei Wang, and Ke Yi <br>
        <strong>ArXiv ID:</strong> 2111.06833 </p>
    </div>
    
    <section>
        <h2>Summary (The Big Picture)</h2>
        <p>
            This paper delivers a significant breakthrough in <strong>Differential Privacy (DP)</strong> by solving the <span class="key-term">Frequency Estimation</span> problem with near-optimal accuracy and minimal communication overhead. It demonstrates that requiring just slightly more than one message per user—specifically, <strong>$1 + o(1)$</strong> messages—is enough to achieve the high utility previously thought only possible in a trusted centralized system.
        </p>
    </section>

    <section>
        <h2>Key Contributions and Results</h2>
        <ul class="content-list">
            <li>
                <strong>Utility Jump:</strong> Achieved a dramatic shift in accuracy, moving from the worst-case polynomial error ($\Omega(n^{1/4})$ in the 1-message model) to a near-optimal logarithmic error ($\omega(1) \cdot O(\log n)$).
            </li>
            <li>
                <strong>Minimal Communication:</strong> The protocol maintains this high accuracy with an expected message count of only $\mathbf{1 + o(1)}$ per user ($o(1)$ approaches zero as the number of users grows).
            </li>
            <li>
                <strong>Scalability:</strong> Introduced an efficient protocol for identifying "heavy hitters" (most frequent items) in vast datasets, using $o(1)$ messages and running time that is only polylogarithmic in the domain size $B$.
            </li>
            <li>
                <strong>Generalization:</strong> Applied these techniques to solve the $B$-dimensional <strong>1-sparse vector summation problem</strong>, achieving optimal Central DP accuracy ($\tilde{O}(n)$) with the same $\mathbf{1 + o(1)}$ message cost.
            </li>
        </ul>
    </section>

   <section>
    <h2>Problem Statment: Minimal Communication, Maximal Utility</h2>
    <p>
        When collecting data from millions of users (like app usage or search trends), we face a fundamental trade-off between <strong>privacy</strong> and <strong>accuracy</strong>. The goal is to maximize statistical utility while maintaining strong Differential Privacy (DP).
    </p>

    <h3>The Privacy-Accuracy Trade-off in Frequency Estimation</h3>
    
    <table>
        <thead>
            <tr>
                <th>Model</th>
                <th>Privacy Guarantee</th>
                <th>Accuracy (Worst-Case Error for FE)</th>
                <th>Communication Cost</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Central DP</strong></td>
                <td>Requires a <strong>trusted</strong> central server.</td>
                <td><strong>Best:</strong> $\tilde{O}(1)$ error (near-constant).</td>
                <td>One message (to trusted collector).</td>
            </tr>
            <tr>
                <td><strong>Local DP</strong></td>
                <td><strong>Strongest:</strong> User privatizes locally.</td>
                <td><strong>Worst:</strong> $\Omega(n)$ error (linear in user count $n$).</td>
                <td>One message (to collector).</td>
            </tr>
            <tr>
                <td><strong>Shuffle Model</strong> (1 Message)</td>
                <td>Uses <strong>untrusted</strong> Shuffler for anonymity boost.</td>
                <td><strong>Hard Limit:</strong> $\Omega(n^{1/4})$ error.</td>
                <td>Exactly one message (to shuffler).</td>
            </tr>
        </tbody>
    </table>
    
    <p class="cite-note">
        The notation $\Omega(n^{1/4})$ represents a <strong>polynomial lower bound</strong> on the worst-case error for the <strong>Frequency Estimation (FE)</strong> problem when protocols are strictly limited to just <strong>one message</strong> per user in the Shuffle Model. This disappointing bound meant the privacy gain of the Shuffle Model was partially offset by poor utility.
    </p>
    
    <h3>The $1+o(1)$ Solution: Beating the Lower Bound</h3>
    <p>
        The core motivation for this paper was to resolve the dilemma presented by the $1$-message limit and determine if <strong>Central DP-like utility</strong> could be achieved with minimal communication overhead. The result proves the barrier is <strong>not</strong> fundamental.
    </p>
    
    <blockquote class="key-term">
        <p>The solution is a protocol where the <em>expected</em> messages per user is <strong>$1 + o(1)$</strong>. This minimal coordination achieves an exponential leap in accuracy:</p>
        <ul>
            <li><strong>The Cost:</strong> Only a vanishingly small fraction of users ($o(1)$) send a second, minimal "control" message. The overhead effectively <em>vanishes</em> as the number of users ($n$) increases.</li>
            <li><strong>The Benefit:</strong> Accuracy moves from the previous hard limit of $\Omega(n^{1/4})$ to the near-optimal logarithmic error of $\omega(1) \cdot O(\log n)$.</li>
        </ul>
    </blockquote>
    
    <p>
        This finding reveals a "sharp phase transition": giving users the slight ability to use a second, anonymous message channel is <strong>exponentially more powerful</strong> than restricting them to strictly one message.
    </p>
</section>



<section id="protocol">
  <h2>Protocol</h2>

 <!-- ========================================================= -->
<!-- 3.1 Balls-into-Bins (corrected: Bernoulli p + k noise users, no shuffler) -->
<!-- ========================================================= -->

<section id="balls-bins">
  <h3>1. Balls-into-Bins (central-DP mode)</h3>

  <p>
    This abstraction models the <strong>statistical target</strong> that later shuffle-model
    protocols aim to emulate. It describes how messages (balls) land in bins under two sources
    of randomness:
  </p>

  <ul>
    <li><strong>n real users (signal)</strong>: each real user has a private value (bin) \(x\). Independently for each user, flip a Bernoulli(\(p\)) coin:
      <ul>
        <li>With probability \(p\): the user places a ball at their true bin (a "true hit").</li>
        <li>With probability \(1-p\): the user places a ball uniformly at random into one of the \(B\) bins (a "fake").</li>
      </ul>
    </li>
    <li><strong>k noise users (pure noise)</strong>: each noise user always places a ball uniformly at random into one of the \(B\) bins.</li>
  </ul>

  <p>
    <strong>Important:</strong> This abstraction<span style="white-space:nowrap"> —</span> unlike the later protocol implementation — does
    <em>not</em> model the shuffler or any local DP randomizer; it is the centralized statistical model of
    signal + controlled random noise (Bernoulli flips + extra noise balls).
  </p>

  <h4>Observed counts and expectation</h4>
  <p>
    Let \(f_x\) be the true number of real users with value \(x\). Let \(C_x\) be the observed total number
    of balls placed in bin \(x\) after all real-user and noise-user placements. Then:
  </p>

  <ul>
    <li>
      Each real user with value \(x\) contributes to \(C_x\) in expectation:
      \[
        p\cdot 1 + (1-p)\cdot\frac{1}{B}.
      \]
      (They either put a true hit with prob \(p\), or a uniform fake with prob \(1-p\).)
    </li>
    <li>
      Each real user whose value is not \(x\) contributes in expectation \((1-p)/B\) (for the fake case).
    </li>
    <li>
      Each of the \(k\) noise users contributes \(1/B\) in expectation to bin \(x\).
    </li>
  </ul>

  <p>
    Summing up, the expected observed count is:
  </p>

  \[
    \boxed{\displaystyle \mathbb{E}[C_x] \;=\; p\,f_x \;+\; (1-p)\frac{n}{B} \;+\; \frac{k}{B}.}
  \]

  <h4>Unbiased estimator for \(f_x\)</h4>
  <p>
    Rearranging the identity above yields the unbiased estimator used by the Analyzer:
  </p>

  \[
    \boxed{\displaystyle \hat f_x \;=\; \frac{C_x \;-\; (1-p)\dfrac{n}{B} \;-\; \dfrac{k}{B}}{p}.}
  \]

  <p>
    Intuition: subtract the expected contribution of uniform fakes (from both real and noise users) and
    then divide by the hit probability \(p\) to undo the thinning introduced by Bernoulli(\(p\)).
  </p>

  <h4>Variance intuition (brief)</h4>
  <p>
    The estimator variance scales roughly like \(\mathrm{Var}(\hat f_x)\approx \mathrm{Var}(C_x)/p^2\).
    Contributions to \(\mathrm{Var}(C_x)\) include:
  </p>
  <ul>
    <li>Signal-bearing randomness from real users with value \(x\): about \(p(1-p) f_x\).</li>
    <li>Fake-placement randomness from other real users: about \((1-p)n/B\) scale.</li>
    <li>Noise-user randomness: about \(k/B\) scale (plus binomial variance factor).</li>
  </ul>

  <p>
    Choosing \(p\) near 1 reduces the inflation from dividing by \(p\) but reduces obfuscation; picking \(k\) controls
    the uniform background level. The paper tunes \(p\) and \(k\) (and how they are realized with \(1+o(1)\)
    messages) to achieve the desired trade-off between privacy emulation and accuracy.
  </p>

 <h4>Informal pseudocode</h4>

<pre class="monospace">
Input:
  - Domain size B
  - n real users with values x_i
  - Bernoulli parameter p
  - k noise users

For each real user i = 1..n:
  Draw b ~ Bernoulli(p)
  If b == 1:
    Place ball at true bin x_i
  Else:
    Place ball at a uniformly random bin in [1..B]

For j = 1..k (noise users):
  Place ball at a uniformly random bin in [1..B]

Analyzer:
  Observe counts C_x for each bin x
  For each bin x:
    Compute:
      f̂_x = ( C_x - (1-p)·n / B - k / B ) / p

Output:
  Estimated frequency vector { f̂_x }
</pre>
<h4>Small numeric example</h4>

<p>
Let <span class="monospace">n = 100</span>,
<span class="monospace">B = 10</span>,
<span class="monospace">p = 0.9</span>,
<span class="monospace">k = 50</span>,
and suppose the true frequency for a particular bin x is:
</p>

\[
f_x = 12
\]

<ul>
  <li>Expected true hits: \( p \cdot f_x = 0.9 \cdot 12 = 10.8 \)</li>
  <li>Expected uniform fakes from real users: \( (1-p)\cdot n/B = 0.1 \cdot 100 / 10 = 1 \)</li>
  <li>Expected noise from k users: \( k/B = 50 / 10 = 5 \)</li>
</ul>

<p>
So the expected observed count is:
</p>

\[
\mathbb{E}[C_x] = 10.8 + 1 + 5 = 16.8
\]

<p>
The estimator used by the analyzer is:
</p>

\[
\hat f_x = \frac{C_x - 1 - 5}{0.9} = \frac{C_x - 6}{0.9}
\]

<p>
If the observed value is \( C_x \approx 16.8 \), then:
</p>

\[
\hat f_x \approx \frac{16.8 - 6}{0.9} = 12
\]

<p>
which correctly recovers the true frequency in expectation.
</p>



</section>

<!-- ========================================================= -->
<!-- 3.2 Protocol for a Small Domain -->
<!-- ========================================================= -->

<section id="small-domain">
  <h3>3.2 Protocol for a Small Domain (Shuffle Model)</h3>

  <p>
    This section describes the frequency estimation protocol for a small discrete
    domain \( \mathcal{D} = [B] \). The protocol operates in the <b>shuffle model</b>
    and consists of two components:
  </p>

  <ul>
    <li><b>Algorithm 2:</b> A <em>local randomizer</em> run independently by each user.</li>
    <li><b>Algorithm 3:</b> A <em>central analyzer</em> that debiases shuffled reports.</li>
  </ul>

  <p>
    Together, these two algorithms achieve <b>\((\varepsilon,\delta)\)-shuffle DP</b>
    while obtaining <b>near-central-model accuracy</b>.
  </p>

  <!-- ============================ -->
  <!-- 3.2.1 Local Randomizer -->
  <!-- ============================ -->

  <h4>3.2.1 Algorithm 2: Local Randomizer</h4>

  <p>
    Each user \( i \) holds a private value \( x_i \in [B] \).
    The user sends exactly <b>one message</b> to the shuffler.
    The local randomizer proceeds as follows:
  </p>

  <div class="box">
    <b>Algorithm 2 (Local Randomizer)</b>
    <ol>
      <li>With probability \( p \), send the true value \( x_i \).</li>
      <li>With probability \( 1 - p \), send a uniformly random value from \( [B] \).</li>
    </ol>
  </div>

  <p>
    The parameter \( p \) is chosen as:
  </p>

  <p style="text-align:center;">
    \[
      p \;=\; \frac{32 \, B \, \ln(2/\delta)}{n \varepsilon^2}.
    \]
  </p>

  <p>
    Intuitively, each truthful report is hidden among a large number of fake
    uniformly random reports after shuffling. Even though the randomizer itself
    is <em>not locally private</em>, the shuffle provides anonymity amplification.
  </p>

  <h5>Small Example</h5>

  <p>
    Let \( B=3 \), domain \( \{A,B,C\} \), and suppose:
  </p>

  <ul>
    <li>User 1 holds \( A \)</li>
    <li>User 2 holds \( B \)</li>
    <li>User 3 holds \( B \)</li>
  </ul>

  <p>
    Each user flips a biased coin with probability \( p \):
  </p>

  <ul>
    <li>If the coin succeeds → send the true value</li>
    <li>If it fails → send a random value from \( \{A,B,C\} \)</li>
  </ul>

  <p>
    After the shuffler permutes all reports, the analyzer only sees a histogram of
    noisy counts.
  </p>

  <!-- ============================ -->
  <!-- 3.2.2 Analyzer -->
  <!-- ============================ -->

  <h4>3.2.2 Algorithm 3: Analyzer (Debiasing Estimator)</h4>

  <p>
    Let \( Y_j \) be the number of shuffled reports equal to value \( j \in [B] \).
    Since reports include both truthful and fake messages, the raw counts are biased.
  </p>

  <p>
    The analyzer computes the unbiased estimator:
  </p>

  <p style="text-align:center;">
    \[
      \hat{f}_j \;=\; \frac{Y_j - (1-p)\frac{n}{B}}{p}.
    \]
  </p>

  <p>
    This removes the expected contribution of the uniform fake reports and rescales
    by the signal probability \( p \).
  </p>

  <!-- ============================ -->
  <!-- Error Bound -->
  <!-- ============================ -->

  <h4>3.2.3 Accuracy and Error Bound</h4>

  <p>
    Let \( f_j \) be the true frequency of value \( j \).
    The estimator \( \hat{f}_j \) satisfies, with probability at least \( 1-\beta \):
  </p>

  <p style="text-align:center;">
    \[
      |\hat{f}_j - f_j|
      \;=\;
      O\!\left(
      \frac{\sqrt{n \log(B/\beta)}}{\varepsilon}
      \right).
    \]
  </p>

  <p>
    This matches the optimal <b>central DP accuracy</b> up to constant factors.
    Thus, the shuffle model achieves <b>near-central accuracy using only one
    message per user</b>.
  </p>

  <h5>Why Near-Central Accuracy Holds</h5>

  <ul>
    <li>
      Each bin count is the sum of:
      <ul>
        <li>Signal contributions (from users who report truthfully)</li>
        <li>Uniform noise contributions (from fake reports)</li>
      </ul>
    </li>

    <li>
      By standard concentration bounds (Chernoff/Hoeffding):
      \[
        Y_j = \mathbf{E}[Y_j] \pm O(\sqrt{n \log(B/\beta)}).
      \]
    </li>

    <li>
      After debiasing (division by \( p \)), the noise scales as:
      \[
        O\!\left(\frac{\sqrt{n \log(B/\beta)}}{p}\right)
        =
        O\!\left(\frac{\sqrt{n \log(B/\beta)}}{\varepsilon}\right).
      \]
    </li>
  </ul>

  <p>
    Thus, Algorithms 2 and 3 together achieve central-level accuracy while
    preserving shuffle-model privacy.
  </p>

<h4>3.2.4 Why the Protocol Improves Accuracy</h4>
<p>
    In the 1-message shuffle model, each user sends only a single report. This restriction
    limits the ability to reduce variance, resulting in a worst-case error of $\Omega(n^{1/4})$.
    By allowing an <strong>expected $1 + o(1)$ messages per user</strong>, a tiny fraction of users send
    extra "control" messages to compensate for randomness and hash collisions.
</p>
<ul>
    <li>The additional messages effectively emulate <strong>central DP noise addition</strong>, 
        without increasing communication overhead significantly.</li>
    <li>As a result, the error bound improves from polynomial to near-logarithmic:
        \[
        |\hat{f}_x - f_x| = O\!\left(\frac{\sqrt{n \log(|\mathcal{X}|/\beta)}}{\varepsilon}\right)
        \]
        which is close to the central DP optimal accuracy.</li>
    <li>This phase transition demonstrates the exponential power of allowing minimal coordination beyond a strict 1-message limit.</li>
</ul>


</section>


<!-- ========================================================= -->
<!-- 3.3 Protocol for a Large Domain -->
<!-- ========================================================= -->

<section id="large-domain">
  <h3>3.3 Protocol for a Large Domain</h3>

  <p>
    When the original domain \( \mathcal{X} \) is extremely large (e.g.,
    \( |\mathcal{X}| = 2^{60} \)), it is computationally infeasible to apply
    the small-domain protocol directly.
  </p>

  <p>
    The key idea of the large-domain protocol is:
  </p>

  <blockquote>
    <b>Reduce the large domain to a small one using hashing,
    then apply the small-domain shuffle protocol as a subroutine.</b>
  </blockquote>

  <p>
    This protocol consists of:
  </p>

  <ul>
    <li><b>Algorithm 4:</b> Hash-based domain reduction</li>
    <li><b>Algorithm 5:</b> Repeated small-domain estimation and aggregation</li>
  </ul>

  <!-- ============================ -->
  <!-- Algorithm 4: Hashing -->
  <!-- ============================ -->

  <h4>3.3.1 Algorithm 4: Hash-Based Domain Reduction</h4>

  <p>
    Each user holds a value \( x_i \in \mathcal{X} \).
    The system publicly samples a random hash function:
  </p>

  <p style="text-align:center;">
    \[
      h : \mathcal{X} \rightarrow [B]
    \]
  </p>

  <p>
    Each user locally computes:
  </p>

  <p style="text-align:center;">
    \[
      y_i = h(x_i)
    \]
  </p>

  <p>
    This compresses the massive original domain into a manageable domain
    of size \( B \).
  </p>

  <p class="note">
    Collisions are unavoidable, but they are random and are controlled
    using repeated independent hash functions.
  </p>

  <!-- ============================ -->
  <!-- Algorithm 5: Repeated Small Domain -->
  <!-- ============================ -->

  <h4>3.3.2 Algorithm 5: Repeated Small-Domain Shuffle Estimation</h4>

  <p>
    The system repeats the following process for
    \( T = O(\log |\mathcal{X}|) \) independent hash functions:
  </p>

  <ol>
    <li>Each user computes \( y_i^{(t)} = h_t(x_i) \)</li>
    <li>Users apply the <b>Small-Domain Local Randomizer (Algorithm 2)</b></li>
    <li>The analyzer applies the <b>Small-Domain Analyzer (Algorithm 3)</b></li>
  </ol>

  <p>
    For each round \( t \), the analyzer recovers:
  </p>

  <p style="text-align:center;">
    \[
      \hat{f}^{(t)}_j \approx |\{ i : h_t(x_i) = j \}|
    \]
  </p>

  <p>
    Across rounds, the estimates are combined using median or averaging
    to cancel hash collisions.
  </p>

  <!-- ============================ -->
  <!-- Accuracy -->
  <!-- ============================ -->

  <h4>3.3.3 Accuracy Guarantee (Large Domain)</h4>

  <p>
    Let \( x \in \mathcal{X} \) be any fixed element.
    Its frequency estimate satisfies:
  </p>

  <p style="text-align:center;">
    \[
      |\hat{f}_x - f_x|
      =
      O\!\left(
      \frac{\sqrt{n \log(|\mathcal{X}|/\beta)}}{\varepsilon}
      \right)
    \]
  </p>

  <p>
    Thus, despite the enormous domain size,
    the protocol achieves the same near-central-DP accuracy as
    the small-domain case, with only a logarithmic overhead.
  </p>

<h4>3.3.4 Advantages of the Large-Domain Protocol</h4>
<ul>
    <li><strong>Scalability:</strong> Handles domains up to $2^{60}$ elements efficiently using hashing and repeated small-domain estimation.</li>
    <li><strong>Robustness:</strong> Repeating with multiple hash functions and combining estimates reduces the impact of hash collisions.</li>
    <li><strong>Privacy Preservation:</strong> Each user still sends an expected $1 + o(1)$ messages, maintaining $(\varepsilon,\delta)$-shuffle DP.</li>
    <li><strong>Near-Central Accuracy:</strong> Achieves the same accuracy as if data were collected centrally with trusted aggregation.</li>
    <li><strong>Minimal Communication:</strong> Extra messages vanish asymptotically, ensuring the protocol remains practical for millions of users.</li>
</ul>



</section>
<!-- ========================================================= -->
<!-- 3.4 Heavy Hitter Identification -->
<!-- ========================================================= -->

<section id="heavy-hitters">
  <h3>3.4 Heavy Hitter Identification in the Shuffle Model</h3>

  <p>
    The heavy hitter problem asks to identify all items
    whose frequencies exceed a threshold:
  </p>

  <p style="text-align:center;">
    \[
      f_x \ge \phi n
    \]
  </p>

  <p>
    The paper solves this using:
  </p>

  <ul>
    <li><b>Algorithm 5:</b> Hashed candidate generation</li>
    <li><b>Algorithm 6:</b> Candidate refinement</li>
    <li><b>Algorithm 7:</b> Final verification via shuffle FE</li>
  </ul>

  <!-- ============================ -->
  <!-- Algorithm 5: Candidate Generation -->
  <!-- ============================ -->

  <h4>3.4.1 Algorithm 5: Hashed Candidate Generation</h4>

  <p>
    The large domain is hashed repeatedly into small domains.
    In each hash table:
  </p>

  <ul>
    <li>Small-domain shuffle FE (Algorithms 2–3) is executed</li>
    <li>Heavy bins (large estimated frequency) are selected</li>
  </ul>

  <p>
    The union of all selected bins across hash tables forms a
    <b>candidate set</b> that contains all true heavy hitters
    with high probability.
  </p>

  <!-- ============================ -->
  <!-- Algorithm 6: Refinement -->
  <!-- ============================ -->

  <h4>3.4.2 Algorithm 6: Candidate Refinement</h4>

  <p>
    Many candidates may be false positives caused by collisions.
    This stage:
  </p>

  <ul>
    <li>Re-hashes candidates into new small domains</li>
    <li>Repeats small-domain FE</li>
    <li>Eliminates elements that fail consistency checks</li>
  </ul>

  <!-- ============================ -->
  <!-- Algorithm 7: Final Verification -->
  <!-- ============================ -->

  <h4>3.4.3 Algorithm 7: Final Heavy Hitter Verification</h4>

  <p>
    Each remaining candidate \( x \) is now treated as a
    <b>single element frequency query</b>.
  </p>

  <p>
    The shuffle protocol is applied one final time to estimate:
  </p>

  <p style="text-align:center;">
    \[
      \hat{f}_x
    \]
  </p>

  <p>
    The item \( x \) is declared a heavy hitter if:
  </p>

  <p style="text-align:center;">
    \[
      \hat{f}_x \ge \phi n
    \]
  </p>

  <!-- ============================ -->
  <!-- Guarantee -->
  <!-- ============================ -->

  <h4>3.4.4 Guarantees</h4>

  <ul>
    <li>All true heavy hitters are recovered with probability \( 1-\beta \)</li>
    <li>No false element exceeds the threshold</li>
    <li>Messages per user: \( o(1) \)</li>
    <li>Runtime: polylogarithmic in \( |\mathcal{X}| \)</li>
  </ul>

<h4>3.4.5 Heavy Hitter Accuracy</h4>
<p>
    The multi-stage heavy hitter protocol ensures that:
</p>
<ul>
    <li>True heavy hitters are always included in the candidate set with high probability.</li>
    <li>False positives from hash collisions are filtered out using refinement and verification.</li>
    <li>The error for estimating the frequency of heavy hitters:
        \[
        |\hat{f}_x - f_x| = O\!\left(\frac{\sqrt{n \log(|\mathcal{X}|/\beta)}}{\varepsilon}\right)
        \]
        which is substantially lower than the 1-message baseline.</li>
    <li>Combining hashing, repeated FE, and the shuffle model allows the protocol to scale gracefully while maintaining both privacy and near-optimal accuracy.</li>
</ul>


</section>

  <section id="dependency-graph">
  <h3>Conceptual Dependency Graph (with reduction methods)</h3>

  <svg width="100%" height="650" viewBox="0 0 500 700" xmlns="http://www.w3.org/2000/svg" role="img" aria-labelledby="dep-title dep-desc" style="background:transparent">
    <title id="dep-title">Protocol dependency and reduction methods</title>
    <desc id="dep-desc">Vertical flow from Input to Heavy Hitters, Large Domain reduction, Small-Domain FE, and Output, with reduction methods labeled on arrows.</desc>

    <defs>
      <style>
        .box { rx:12; fill:#ffffff; stroke:#e6eefc; stroke-width:2; filter: drop-shadow(0 4px 12px rgba(2,132,199,0.08)); font-family:Inter, system-ui; }
        .input { fill:#f0f9ff; stroke:#0284c7; }
        .heavy { fill:#fff7ed; stroke:#fb923c; }
        .large { fill:#ecfdf5; stroke:#16a34a; }
        .small { fill:#ecfeff; stroke:#0284c7; }
        .output { fill:#fffbeb; stroke:#f59e0b; }
        .title { font-weight:700; font-size:14px; fill:#0f172a; }
        .sub { font-size:12px; fill:#334155; }
        .method { font-size:12px; fill:#475569; font-style:italic; text-anchor:start; }
        .arrow { stroke:#0f172a; stroke-width:2; fill:none; marker-end: url(#arrowhead); }
      </style>

      <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
        <path d="M0 0 L10 5 L0 10 z" fill="#0f172a"/>
      </marker>
    </defs>

    <!-- Input box -->
    <g transform="translate(150,20)">
      <rect class="box input" width="200" height="70"/>
      <text x="100" y="30" text-anchor="middle" class="title">Input</text>
      <text x="100" y="50" text-anchor="middle" class="sub">Massive domain</text>
    </g>

    <!-- Arrow: Input → Heavy Hitters -->
    <path class="arrow" d="M250 90 L250 150"/>
    <text x="260" y="120" class="method">Candidate generation: hashing + subsampling</text>

    <!-- Heavy Hitters box -->
    <g transform="translate(150,150)">
      <rect class="box heavy" width="200" height="90"/>
      <text x="100" y="30" text-anchor="middle" class="title">Heavy Hitters</text>
      <text x="100" y="55" text-anchor="middle" class="sub">Identify frequent items</text>
      <text x="100" y="75" text-anchor="middle" class="method">Algs. 6–8</text>
    </g>

    <!-- Arrow: Heavy → Large -->
    <path class="arrow" d="M250 240 L250 300"/>
    <text x="260" y="270" class="method">Random hashing + repetition</text>

    <!-- Large Domain box -->
    <g transform="translate(150,300)">
      <rect class="box large" width="200" height="90"/>
      <text x="100" y="30" text-anchor="middle" class="title">Large Domain Reduction</text>
      <text x="100" y="55" text-anchor="middle" class="sub">Compress domain into buckets</text>
      <text x="100" y="75" text-anchor="middle" class="method">Algs. 4–5</text>
    </g>

    <!-- Arrow: Large → Small -->
    <path class="arrow" d="M250 390 L250 450"/>
    <text x="260" y="420" class="method">Hashing / domain reduction</text>

    <!-- Small-Domain FE box -->
    <g transform="translate(150,450)">
      <rect class="box small" width="200" height="90"/>
      <text x="100" y="30" text-anchor="middle" class="title">Small-Domain FE</text>
      <text x="100" y="55" text-anchor="middle" class="sub">Shuffle + analyzer</text>
      <text x="100" y="75" text-anchor="middle" class="method">Algs. 2–3</text>
    </g>

    <!-- Arrow: Small → Output -->
    <path class="arrow" d="M250 540 L250 590"/>
    <text x="260" y="565" class="method">Aggregate results</text>

    <!-- Output box -->
    <g transform="translate(150,590)">
      <rect class="box output" width="200" height="70"/>
      <text x="100" y="30" text-anchor="middle" class="title">Output</text>
      <text x="100" y="50" text-anchor="middle" class="sub">Frequency estimates</text>
    </g>

  </svg>
</section>


<section id="future-work">
  <h2>Potential Future Work</h2>
  <ul>
      
  </ul>
</section>


    <!-- Footer and Citations -->
    <footer>
        <p class="cite-note">Citations: <br>
        [1] Qiyao Luo, Yilei Wang, Ke Yi (2021). <a href="https://arxiv.org/abs/2111.06833">Frequency Estimation in the Shuffle Model with Almost a Single Message</a>. <br>
        </p>
    </footer>

</body>
</html>
